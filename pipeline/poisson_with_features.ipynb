{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2acc22f",
   "metadata": {},
   "source": [
    "# 1. Imports & Station List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465febf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_STATIONS = [\n",
    "    \"MIT at Mass Ave / Amherst St\",\n",
    "    \"Central Square at Mass Ave / Essex St\",\n",
    "    \"MIT Pacific St at Purrington St\",\n",
    "    \"Harvard Square at Mass Ave / Dunster St\",\n",
    "    \"Boylston St at Massachusetts Ave\",\n",
    "    \"Charles St at Cambridge St\",\n",
    "    \"Forsyth St at Huntington Ave\",\n",
    "    \"Boylston St at Fairfield St\",\n",
    "    \"Christian Science Plaza - Massachusetts Ave at Westland Ave\",\n",
    "    \"MIT Stata Center at Vassar St / Main St\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde6348",
   "metadata": {},
   "source": [
    "# 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. LOAD 12 MONTHS OF BLUEBIKES TRIP DATA\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "tripdata_path = \"2024_data/\"   # <-- change to your folder\n",
    "\n",
    "all_files = glob.glob(tripdata_path + \"2024*-bluebikes-tripdata.csv\")\n",
    "\n",
    "print(\"Files detected:\")\n",
    "for f in all_files:\n",
    "    print(f)\n",
    "\n",
    "df_list = []\n",
    "for filename in all_files:\n",
    "    print(\"Loading:\", filename)\n",
    "    df_month = pd.read_csv(filename)\n",
    "    df_list.append(df_month)\n",
    "\n",
    "rides = pd.concat(df_list, ignore_index=True)\n",
    "print(\"Total rows loaded:\", len(rides))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4fbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. LOAD STATION FEATURE DATA (feature.csv)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "station_features = pd.read_csv(\"feature.csv\")   # <-- your uploaded file\n",
    "\n",
    "# Rename columns to shorter / code-friendly names\n",
    "station_features = station_features.rename(columns={\n",
    "    \"Station_id\": \"station_id\",\n",
    "    \"Station_name\": \"station_name\",\n",
    "    \"Station latitude\": \"station_lat\",\n",
    "    \"Station longitude\": \"station_lng\",\n",
    "    \"Distance to nearest subway stop (m)\": \"dist_subway_m\",\n",
    "    \"Distance to nearest bus stop (m)\": \"dist_bus_m\",\n",
    "    \"Distance to nearest university (m)\": \"dist_university_m\",\n",
    "    \"Distance to nearest business district\": \"dist_business\",\n",
    "    \"Distance to nearest residential area\": \"dist_residential\",\n",
    "    \"Population density around station\": \"pop_density\",\n",
    "    \"Employment density around station\": \"emp_density\",\n",
    "    \"Number of restaurants/cafes nearby\": \"restaurant_count\",\n",
    "    \"Restaurant/cafes density around station\": \"restaurant_density\",\n",
    "})\n",
    "\n",
    "# This file is purely station-level (no time info), so we just keep the static columns.\n",
    "station_features = station_features[[\n",
    "    \"station_name\",\n",
    "    \"station_lat\",\n",
    "    \"station_lng\",\n",
    "    \"dist_subway_m\",\n",
    "    \"dist_bus_m\",\n",
    "    \"dist_university_m\",\n",
    "    \"dist_business\",\n",
    "    \"dist_residential\",\n",
    "    \"pop_density\",\n",
    "    \"emp_density\",\n",
    "    \"restaurant_count\",\n",
    "    \"restaurant_density\",\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e32122",
   "metadata": {},
   "source": [
    "## 2.1 Build hourly inflow / outflow per station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fcaab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides[\"started_at\"] = pd.to_datetime(rides[\"started_at\"], format=\"mixed\")\n",
    "rides[\"ended_at\"] = pd.to_datetime(rides[\"ended_at\"], format=\"mixed\")\n",
    "\n",
    "\n",
    "# Floor to hour for aggregation\n",
    "rides[\"start_hour\"] = rides[\"started_at\"].dt.floor(\"H\")\n",
    "rides[\"end_hour\"] = rides[\"ended_at\"].dt.floor(\"H\")\n",
    "\n",
    "# Outflow: bikes leaving the station\n",
    "out_df = (\n",
    "    rides[rides[\"start_station_name\"].isin(TARGET_STATIONS)]\n",
    "    .groupby([\"start_station_name\", \"start_hour\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"out_count\")\n",
    "    .rename(columns={\"start_station_name\": \"station_name\",\n",
    "                     \"start_hour\": \"hour\"})\n",
    ")\n",
    "\n",
    "# Inflow: bikes arriving at the station\n",
    "in_df = (\n",
    "    rides[rides[\"end_station_name\"].isin(TARGET_STATIONS)]\n",
    "    .groupby([\"end_station_name\", \"end_hour\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"in_count\")\n",
    "    .rename(columns={\"end_station_name\": \"station_name\",\n",
    "                     \"end_hour\": \"hour\"})\n",
    ")\n",
    "\n",
    "# Merge inflow & outflow into one panel\n",
    "panel = pd.merge(\n",
    "    out_df,\n",
    "    in_df,\n",
    "    how=\"outer\",\n",
    "    on=[\"station_name\", \"hour\"]\n",
    ")\n",
    "\n",
    "# Replace NaNs with 0 (no trips that hour)\n",
    "panel[[\"in_count\", \"out_count\"]] = panel[[\"in_count\", \"out_count\"]].fillna(0).astype(int)\n",
    "\n",
    "panel.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d15a5f",
   "metadata": {},
   "source": [
    "## 2.2 Merge with station-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the columns you really want from station_features\n",
    "station_features_clean = station_features[[\n",
    "    \"station_name\",\n",
    "    \"station_lat\",\n",
    "    \"station_lng\",\n",
    "    \"dist_subway_m\",\n",
    "    \"dist_bus_m\",\n",
    "    \"dist_university_m\",\n",
    "    \"dist_business\",\n",
    "    \"dist_residential\",\n",
    "    \"pop_density\",\n",
    "    \"emp_density\",\n",
    "    \"restaurant_count\",\n",
    "    \"restaurant_density\",\n",
    "]]\n",
    "\n",
    "# Merge\n",
    "panel = panel.merge(\n",
    "    station_features_clean,\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "#Time-based features\n",
    "panel[\"month\"] = panel[\"hour\"].dt.month\n",
    "panel[\"day_of_week\"] = panel[\"hour\"].dt.dayofweek    # 0 = Monday\n",
    "panel[\"hour_of_day\"] = panel[\"hour\"].dt.hour\n",
    "panel[\"is_weekend\"] = panel[\"day_of_week\"].isin([5, 6]).astype(int)  # Sat/Sun\n",
    "\n",
    "\n",
    "\n",
    "panel.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8409e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_features.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feceda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns\n",
    "numeric_features = [\n",
    "    \"hour_of_day\",\n",
    "    \"day_of_week\",\n",
    "    \"month\",\n",
    "    \"is_weekend\",\n",
    "    \"station_lat\",\n",
    "    \"station_lng\",\n",
    "    \"dist_subway_m\",\n",
    "    \"dist_bus_m\",\n",
    "    \"dist_university_m\",\n",
    "    \"dist_business\",\n",
    "    \"dist_residential\",\n",
    "    \"restaurant_count\",\n",
    "]\n",
    "\n",
    "X = panel[numeric_features].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f38944",
   "metadata": {},
   "source": [
    "# 3.Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. Extract target variables\n",
    "# ---------------------------------------------------------\n",
    "y_out = panel[\"out_count\"].values   # shape (n_samples,)\n",
    "y_in  = panel[\"in_count\"].values    # shape (n_samples,)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Combine out & in into ONE target matrix\n",
    "#    Y[:,0] = out_count\n",
    "#    Y[:,1] = in_count\n",
    "# ---------------------------------------------------------\n",
    "Y = np.column_stack([y_out, y_in])   # shape (n_samples, 2)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Train-Test Split (same split for both outputs)\n",
    "# ---------------------------------------------------------\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Extract individual outputs after split\n",
    "# ---------------------------------------------------------\n",
    "y_train_out = Y_train[:, 0]\n",
    "y_train_in  = Y_train[:, 1]\n",
    "\n",
    "y_test_out  = Y_test[:, 0]\n",
    "y_test_in   = Y_test[:, 1]\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"Y_test:\", Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9f7d7",
   "metadata": {},
   "source": [
    "# 4. Build a Poisson regression pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b826b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: scale numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Poisson regression model\n",
    "poisson_model = PoissonRegressor(alpha=1.0, max_iter=1000)\n",
    "\n",
    "# Full pipeline: preprocessing + model\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", poisson_model)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb69f9",
   "metadata": {},
   "source": [
    "# 5. Fit the model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "# Pipeline: impute → scale → Poisson regression\n",
    "clf = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"poisson\", PoissonRegressor(alpha=1e-4, max_iter=300))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "clf.fit(X_train, y_train_in)\n",
    "\n",
    "# Predict\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred  = clf.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "def print_regression_metrics(split, y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f\"=== {split} ===\")\n",
    "    print(f\"MAE:  {mae:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "\n",
    "print_regression_metrics(\"TRAIN\", y_train_in, y_train_pred)\n",
    "print_regression_metrics(\"TEST\", y_test_in, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98308834",
   "metadata": {},
   "source": [
    "# 6. Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# ---- choose your demand threshold for classification ----\n",
    "threshold = 4\n",
    "\n",
    "# convert to binary classes\n",
    "y_train_true_bin = (y_train_in >= threshold).astype(int)\n",
    "y_train_pred_bin = (y_train_pred >= threshold).astype(int)\n",
    "\n",
    "y_test_true_bin  = (y_test_in >= threshold).astype(int)\n",
    "y_test_pred_bin  = (y_test_pred >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "best_f1 = 0\n",
    "best_t = None\n",
    "\n",
    "for t in range(1, 40):\n",
    "    pred_bin = (y_test_pred >= t).astype(int)\n",
    "    f1 = f1_score(y_test_true_bin, pred_bin)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_t = t\n",
    "\n",
    "print(\"Best threshold:\", best_t)\n",
    "print(\"Best F1:\", best_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590aedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_metrics(split, y_true, y_pred):\n",
    "    print(f\"\\n=== {split} — Classification Metrics ===\")\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall:   \", recall_score(y_true, y_pred))\n",
    "    print(\"F1 Score: \", f1_score(y_true, y_pred))\n",
    "\n",
    "print_classification_metrics(\"TRAIN\", y_train_true_bin, y_train_pred_bin)\n",
    "print_classification_metrics(\"TEST\", y_test_true_bin, y_test_pred_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion(split, y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Pred Low\", \"Pred High\"],\n",
    "                yticklabels=[\"True Low\", \"True High\"])\n",
    "    plt.title(f\"{split} Confusion Matrix\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion(\"TRAIN\", y_train_true_bin, y_train_pred_bin)\n",
    "plot_confusion(\"TEST\", y_test_true_bin, y_test_pred_bin)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
