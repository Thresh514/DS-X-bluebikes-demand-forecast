{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2acc22f",
   "metadata": {},
   "source": [
    "# 1. Imports & Station List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465febf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7244c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_STATIONS = [\n",
    "    \"MIT at Mass Ave / Amherst St\",\n",
    "    \"Central Square at Mass Ave / Essex St\",\n",
    "    \"MIT Pacific St at Purrington St\",\n",
    "    \"Harvard Square at Mass Ave / Dunster St\",\n",
    "    \"Boylston St at Massachusetts Ave\",\n",
    "    \"Charles St at Cambridge St\",\n",
    "    \"Forsyth St at Huntington Ave\",\n",
    "    \"Boylston St at Fairfield St\",\n",
    "    \"Christian Science Plaza - Massachusetts Ave at Westland Ave\",\n",
    "    \"MIT Stata Center at Vassar St / Main St\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f2b9c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fde6348",
   "metadata": {},
   "source": [
    "# 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603e9cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files detected:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     df_month = pd.read_csv(filename)\n\u001b[32m     17\u001b[39m     df_list.append(df_month)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m rides = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTotal rows loaded:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(rides))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/25Fall/CS506/bluebikes-demand-forecast/venv/lib/python3.12/site-packages/pandas/core/reshape/concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/25Fall/CS506/bluebikes-demand-forecast/venv/lib/python3.12/site-packages/pandas/core/reshape/concat.py:445\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m.verify_integrity = verify_integrity\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.copy = copy\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m objs, keys = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    448\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/25Fall/CS506/bluebikes-demand-forecast/venv/lib/python3.12/site-packages/pandas/core/reshape/concat.py:507\u001b[39m, in \u001b[36m_Concatenator._clean_keys_and_objs\u001b[39m\u001b[34m(self, objs, keys)\u001b[39m\n\u001b[32m    504\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    510\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(com.not_none(*objs_list))\n",
      "\u001b[31mValueError\u001b[39m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. LOAD 12 MONTHS OF BLUEBIKES TRIP DATA\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "tripdata_path = \"2024_data/\"   # <-- change to your folder\n",
    "\n",
    "all_files = glob.glob(tripdata_path + \"2024*-bluebikes-tripdata.csv\")\n",
    "\n",
    "print(\"Files detected:\")\n",
    "for f in all_files:\n",
    "    print(f)\n",
    "\n",
    "df_list = []\n",
    "for filename in all_files:\n",
    "    print(\"Loading:\", filename)\n",
    "    df_month = pd.read_csv(filename)\n",
    "    df_list.append(df_month)\n",
    "\n",
    "rides = pd.concat(df_list, ignore_index=True)\n",
    "print(\"Total rows loaded:\", len(rides))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4fbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. LOAD STATION FEATURE DATA (feature.csv)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "station_features = pd.read_csv(\"feature.csv\")   # <-- your uploaded file\n",
    "\n",
    "# Rename columns to shorter / code-friendly names\n",
    "station_features = station_features.rename(columns={\n",
    "    \"Station_id\": \"station_id\",\n",
    "    \"Station_name\": \"station_name\",\n",
    "    \"Station latitude\": \"station_lat\",\n",
    "    \"Station longitude\": \"station_lng\",\n",
    "    \"Distance to nearest subway stop (m)\": \"dist_subway_m\",\n",
    "    \"Distance to nearest bus stop (m)\": \"dist_bus_m\",\n",
    "    \"Distance to nearest university (m)\": \"dist_university_m\",\n",
    "    \"Distance to nearest business district\": \"dist_business\",\n",
    "    \"Distance to nearest residential area\": \"dist_residential\",\n",
    "    \"Population density around station\": \"pop_density\",\n",
    "    \"Employment density around station\": \"emp_density\",\n",
    "    \"Number of restaurants/cafes nearby\": \"restaurant_count\",\n",
    "    \"Restaurant/cafes density around station\": \"restaurant_density\",\n",
    "})\n",
    "\n",
    "# This file is purely station-level (no time info), so we just keep the static columns.\n",
    "station_features = station_features[[\n",
    "    \"station_name\",\n",
    "    \"station_lat\",\n",
    "    \"station_lng\",\n",
    "    \"dist_subway_m\",\n",
    "    \"dist_bus_m\",\n",
    "    \"dist_university_m\",\n",
    "    \"dist_business\",\n",
    "    \"dist_residential\",\n",
    "    \"pop_density\",\n",
    "    \"emp_density\",\n",
    "    \"restaurant_count\",\n",
    "    \"restaurant_density\",\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e32122",
   "metadata": {},
   "source": [
    "## 2.1 Build hourly inflow / outflow per station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fcaab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides[\"started_at\"] = pd.to_datetime(rides[\"started_at\"], format=\"mixed\")\n",
    "rides[\"ended_at\"] = pd.to_datetime(rides[\"ended_at\"], format=\"mixed\")\n",
    "\n",
    "\n",
    "# Floor to hour for aggregation\n",
    "rides[\"start_hour\"] = rides[\"started_at\"].dt.floor(\"H\")\n",
    "rides[\"end_hour\"] = rides[\"ended_at\"].dt.floor(\"H\")\n",
    "\n",
    "# Outflow: bikes leaving the station\n",
    "out_df = (\n",
    "    rides[rides[\"start_station_name\"].isin(TARGET_STATIONS)]\n",
    "    .groupby([\"start_station_name\", \"start_hour\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"out_count\")\n",
    "    .rename(columns={\"start_station_name\": \"station_name\",\n",
    "                     \"start_hour\": \"hour\"})\n",
    ")\n",
    "\n",
    "# Inflow: bikes arriving at the station\n",
    "in_df = (\n",
    "    rides[rides[\"end_station_name\"].isin(TARGET_STATIONS)]\n",
    "    .groupby([\"end_station_name\", \"end_hour\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"in_count\")\n",
    "    .rename(columns={\"end_station_name\": \"station_name\",\n",
    "                     \"end_hour\": \"hour\"})\n",
    ")\n",
    "\n",
    "# Merge inflow & outflow into one panel\n",
    "panel = pd.merge(\n",
    "    out_df,\n",
    "    in_df,\n",
    "    how=\"outer\",\n",
    "    on=[\"station_name\", \"hour\"]\n",
    ")\n",
    "\n",
    "# Replace NaNs with 0 (no trips that hour)\n",
    "panel[[\"in_count\", \"out_count\"]] = panel[[\"in_count\", \"out_count\"]].fillna(0).astype(int)\n",
    "\n",
    "panel.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d15a5f",
   "metadata": {},
   "source": [
    "## 2.2 Merge with station-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the columns you really want from station_features\n",
    "station_features_clean = station_features[[\n",
    "    \"station_name\",\n",
    "    \"station_lat\",\n",
    "    \"station_lng\",\n",
    "    \"dist_subway_m\",\n",
    "    \"dist_bus_m\",\n",
    "    \"dist_university_m\",\n",
    "    \"dist_business\",\n",
    "    \"dist_residential\",\n",
    "    \"pop_density\",\n",
    "    \"emp_density\",\n",
    "    \"restaurant_count\",\n",
    "    \"restaurant_density\",\n",
    "]]\n",
    "\n",
    "# Merge\n",
    "panel = panel.merge(\n",
    "    station_features_clean,\n",
    "    on=\"station_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "#Time-based features\n",
    "panel[\"month\"] = panel[\"hour\"].dt.month\n",
    "panel[\"day_of_week\"] = panel[\"hour\"].dt.dayofweek    # 0 = Monday\n",
    "panel[\"hour_of_day\"] = panel[\"hour\"].dt.hour\n",
    "panel[\"is_weekend\"] = panel[\"day_of_week\"].isin([5, 6]).astype(int)  # Sat/Sun\n",
    "\n",
    "\n",
    "\n",
    "panel.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8409e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_features.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feceda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns\n",
    "numeric_features = [\n",
    "    \"hour_of_day\",\n",
    "    \"day_of_week\",\n",
    "    \"month\",\n",
    "    \"is_weekend\",\n",
    "    \"station_lat\",\n",
    "    \"station_lng\",\n",
    "    \"dist_subway_m\",\n",
    "    \"dist_bus_m\",\n",
    "    \"dist_university_m\",\n",
    "    \"dist_business\",\n",
    "    \"dist_residential\",\n",
    "    \"restaurant_count\",\n",
    "]\n",
    "\n",
    "X = panel[numeric_features].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f38944",
   "metadata": {},
   "source": [
    "# 3.Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. Extract target variables\n",
    "# ---------------------------------------------------------\n",
    "y_out = panel[\"out_count\"].values   # shape (n_samples,)\n",
    "y_in  = panel[\"in_count\"].values    # shape (n_samples,)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Combine out & in into ONE target matrix\n",
    "#    Y[:,0] = out_count\n",
    "#    Y[:,1] = in_count\n",
    "# ---------------------------------------------------------\n",
    "Y = np.column_stack([y_out, y_in])   # shape (n_samples, 2)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Train-Test Split (same split for both outputs)\n",
    "# ---------------------------------------------------------\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Extract individual outputs after split\n",
    "# ---------------------------------------------------------\n",
    "y_train_out = Y_train[:, 0]\n",
    "y_train_in  = Y_train[:, 1]\n",
    "\n",
    "y_test_out  = Y_test[:, 0]\n",
    "y_test_in   = Y_test[:, 1]\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"Y_test:\", Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9f7d7",
   "metadata": {},
   "source": [
    "# 4. Build a Poisson regression pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b826b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: scale numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Poisson regression model\n",
    "poisson_model = PoissonRegressor(alpha=1.0, max_iter=1000)\n",
    "\n",
    "# Full pipeline: preprocessing + model\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", poisson_model)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb69f9",
   "metadata": {},
   "source": [
    "# 5. Fit the model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1. Impute missing values\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test)\n",
    "\n",
    "# 2. Add intercept for statsmodels\n",
    "X_train_sm = sm.add_constant(X_train_imp)\n",
    "X_test_sm  = sm.add_constant(X_test_imp)\n",
    "\n",
    "# 3. Fit Negative Binomial model\n",
    "model_nb = sm.GLM(\n",
    "    y_train_in,\n",
    "    X_train_sm,\n",
    "    family=sm.families.NegativeBinomial()\n",
    ").fit()\n",
    "\n",
    "print(model_nb.summary())\n",
    "\n",
    "# 4. Predict\n",
    "y_train_pred = model_nb.predict(X_train_sm)\n",
    "y_test_pred  = model_nb.predict(X_test_sm)\n",
    "\n",
    "def print_regression_metrics(split, y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f\"=== {split} ===\")\n",
    "    print(f\"MAE:  {mae:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "\n",
    "# 5. Evaluate\n",
    "print_regression_metrics(\"TRAIN\", y_train_in, y_train_pred)\n",
    "print_regression_metrics(\"TEST\", y_test_in, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98308834",
   "metadata": {},
   "source": [
    "# 6. Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# ---- choose your demand threshold for classification ----\n",
    "threshold = 4\n",
    "\n",
    "# convert to binary classes\n",
    "y_train_true_bin = (y_train_in >= threshold).astype(int)\n",
    "y_train_pred_bin = (y_train_pred >= threshold).astype(int)\n",
    "\n",
    "y_test_true_bin  = (y_test_in >= threshold).astype(int)\n",
    "y_test_pred_bin  = (y_test_pred >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "best_f1 = 0\n",
    "best_t = None\n",
    "\n",
    "for t in range(1, 40):\n",
    "    pred_bin = (y_test_pred >= t).astype(int)\n",
    "    f1 = f1_score(y_test_true_bin, pred_bin)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_t = t\n",
    "\n",
    "print(\"Best threshold:\", best_t)\n",
    "print(\"Best F1:\", best_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590aedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_metrics(split, y_true, y_pred):\n",
    "    print(f\"\\n=== {split} â€” Classification Metrics ===\")\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall:   \", recall_score(y_true, y_pred))\n",
    "    print(\"F1 Score: \", f1_score(y_true, y_pred))\n",
    "\n",
    "print_classification_metrics(\"TRAIN\", y_train_true_bin, y_train_pred_bin)\n",
    "print_classification_metrics(\"TEST\", y_test_true_bin, y_test_pred_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion(split, y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Pred Low\", \"Pred High\"],\n",
    "                yticklabels=[\"True Low\", \"True High\"])\n",
    "    plt.title(f\"{split} Confusion Matrix\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion(\"TRAIN\", y_train_true_bin, y_train_pred_bin)\n",
    "plot_confusion(\"TEST\", y_test_true_bin, y_test_pred_bin)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
