{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16756,
     "status": "ok",
     "timestamp": 1761603438799,
     "user": {
      "displayName": "Matthew He Yan",
      "userId": "17059746695771188026"
     },
     "user_tz": 240
    },
    "id": "3Ei50RnIbAmX",
    "outputId": "91646e27-d641-4263-c071-5bec5fa1a065"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab env\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# input_dir  = Path(\"/content/drive/MyDrive/Side hustles/DS+X/Data/Raw\")\n",
    "# output_dir = Path(\"/content/drive/MyDrive/Side hustles/DS+X/Data/Transformed\")\n",
    "# output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Local env\n",
    "input_dir  = Path(\"2023_data/Bluebikes\")\n",
    "weather_dir = Path(\"2023_data/Weather\")\n",
    "output_dir = Path(\"2023_data/Transformed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1761603465699,
     "user": {
      "displayName": "Matthew He Yan",
      "userId": "17059746695771188026"
     },
     "user_tz": 240
    },
    "id": "AV4UDjZbbMj_"
   },
   "outputs": [],
   "source": [
    "def load_raw_data(csv_path):\n",
    "    df = pd.read_csv(csv_path,\n",
    "                    dtype={\"started_at\": \"string\", \"ended_at\": \"string\"},\n",
    "                    keep_default_na=True,)\n",
    "    return df\n",
    "\n",
    "def parse_bike_time(series: pd.Series, freq: str = \"h\") -> pd.Series:\n",
    "    s = series.astype(str).str.strip().str.replace(\"\\u200b\", \"\", regex=False)\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    # fix single-digit hour \" 1:\" -> \" 01:\" if needed\n",
    "    m = dt.isna()\n",
    "    if m.any():\n",
    "        s2 = s[m].str.replace(r\" (\\d):\", lambda x: f\" 0{x.group(1)}:\", regex=True)\n",
    "        dt.loc[m] = pd.to_datetime(s2, format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
    "    return dt.dt.floor(freq)\n",
    "\n",
    "def transform_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # ---------- Drop rows with missing column data ----------\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # ---------- Standardize column names ----------\n",
    "    df = df.rename(columns={\n",
    "        \"starttime\": \"start_time\",\n",
    "        \"stoptime\": \"stop_time\",\n",
    "        \"start station id\": \"start_station_id\",\n",
    "        \"start station name\": \"start_station_name\",\n",
    "        \"start station latitude\": \"start_station_latitude\",\n",
    "        \"start station longitude\": \"start_station_longitude\",\n",
    "        \"end station id\": \"end_station_id\",\n",
    "        \"end station name\": \"end_station_name\",\n",
    "        \"end station latitude\": \"end_station_latitude\",\n",
    "        \"end station longitude\": \"end_station_longitude\",\n",
    "    })\n",
    "    \n",
    "    df = df.rename(columns={\n",
    "        \"started_at\": \"start_time\",\n",
    "        \"ended_at\": \"stop_time\",\n",
    "        \"start_station_id\": \"start_station_id\",\n",
    "        \"start_station_name\": \"start_station_name\",\n",
    "        \"start_lat\": \"start_station_latitude\",\n",
    "        \"start_lng\": \"start_station_longitude\",\n",
    "        \"end_station_id\": \"end_station_id\",\n",
    "        \"end_station_name\": \"end_station_name\",\n",
    "        \"end_lat\": \"end_station_latitude\",\n",
    "        \"end_lng\": \"end_station_longitude\",\n",
    "    })\n",
    "    \n",
    "    print(\"Columns after renaming:\", df.columns.tolist())\n",
    "    \n",
    "    # ---------- Normalize timestamps to the hour ----------\n",
    "    df[\"start_time\"] = parse_bike_time(df[\"start_time\"], freq=\"h\")\n",
    "    df[\"stop_time\"]  = parse_bike_time(df[\"stop_time\"],  freq=\"h\")\n",
    "    \n",
    "    # ---------- Filter out trips longer than 24 hours ----------\n",
    "    df[\"trip_duration\"] = (df[\"stop_time\"] - df[\"start_time\"]).dt.total_seconds() / 3600  # duration in hours\n",
    "    df = df[df[\"trip_duration\"] <= 24].copy()\n",
    "    df = df.drop(columns=[\"trip_duration\"])\n",
    "\n",
    "    # ---------- Organize location data ----------\n",
    "    start_meta = df[[\"start_station_id\", \"start_station_name\", \"start_station_latitude\", \"start_station_longitude\"]]\n",
    "    end_meta = df[[\"end_station_id\", \"end_station_name\", \"end_station_latitude\", \"end_station_longitude\"]]\n",
    "    \n",
    "    # Rename to common column names for concatenation\n",
    "    start_meta = start_meta.rename(columns={\n",
    "        \"start_station_id\": \"station_id\",\n",
    "        \"start_station_name\": \"station_name\",\n",
    "        \"start_station_latitude\": \"latitude\",\n",
    "        \"start_station_longitude\": \"longitude\",\n",
    "    })\n",
    "    end_meta = end_meta.rename(columns={\n",
    "        \"end_station_id\": \"station_id\",\n",
    "        \"end_station_name\": \"station_name\",\n",
    "        \"end_station_latitude\": \"latitude\",\n",
    "        \"end_station_longitude\": \"longitude\",\n",
    "    })\n",
    "\n",
    "    stations = pd.concat([start_meta, end_meta], ignore_index=True)\n",
    "    stations = stations[stations[\"station_id\"].str.strip() != \"\"]\n",
    "    stations = stations.dropna(subset=[\"station_id\"]).drop_duplicates(\"station_id\")\n",
    "\n",
    "    # ---------- Count OUTS (undocks) by hour & start station ----------\n",
    "    outs = (\n",
    "        df.dropna(subset=[\"start_station_id\"])\n",
    "          .groupby([\"start_time\", \"start_station_id\"], as_index=False)\n",
    "          .size()\n",
    "          .rename(columns={\"start_time\": \"timestart\", \"start_station_id\": \"station_id\", \"size\": \"out\"})\n",
    "    )\n",
    "\n",
    "    # ---------- Count INS (docks) by hour & end station ----------\n",
    "    ins = (\n",
    "        df.dropna(subset=[\"end_station_id\"])\n",
    "          .groupby([\"stop_time\", \"end_station_id\"], as_index=False)\n",
    "          .size()\n",
    "          .rename(columns={\"stop_time\": \"timestart\", \"end_station_id\": \"station_id\", \"size\": \"in\"})\n",
    "    )\n",
    "\n",
    "    # ---------- Combine (outer join so hours with only in OR only out are kept) ----------\n",
    "    hourly = (\n",
    "        pd.merge(outs, ins, on=[\"timestart\", \"station_id\"], how=\"outer\")\n",
    "          .fillna({\"in\": 0, \"out\": 0})\n",
    "    )\n",
    "\n",
    "    # ---------- (Optional) Complete grid: every station × every hour ----------\n",
    "    start_hour = min(df[\"start_time\"].min(), df[\"stop_time\"].min())\n",
    "    end_hour = max(df[\"start_time\"].max(), df[\"stop_time\"].max())\n",
    "    all_hours = pd.date_range(start=start_hour, end=end_hour, freq=\"h\")\n",
    "\n",
    "    all_station_ids = (\n",
    "        stations[\"station_id\"]\n",
    "        .dropna()\n",
    "        .unique()\n",
    "    )\n",
    "\n",
    "    full_idx = pd.MultiIndex.from_product(\n",
    "        [all_hours, all_station_ids],\n",
    "        names=[\"timestart\", \"station_id\"]\n",
    "    )\n",
    "\n",
    "    hourly = (\n",
    "        hourly.set_index([\"timestart\", \"station_id\"])\n",
    "              .reindex(full_idx, fill_value=0)\n",
    "              .reset_index()\n",
    "    )\n",
    "\n",
    "    hourly[\"in\"]  = hourly[\"in\"].astype(\"int64\")\n",
    "    hourly[\"out\"] = hourly[\"out\"].astype(\"int64\")\n",
    "\n",
    "    # ---------- Attach metadata & compute timeend ----------\n",
    "    hourly = (\n",
    "        hourly.merge(stations, on=\"station_id\", how=\"left\")\n",
    "              .assign(timeend=lambda x: x[\"timestart\"] + pd.Timedelta(hours=1))\n",
    "    )\n",
    "\n",
    "    # ---------- Add time-based features from the interval ----------\n",
    "    month_num = hourly[\"timestart\"].dt.month.astype(\"int8\")\n",
    "    day_of_week = hourly[\"timestart\"].dt.dayofweek.astype(\"int8\") + 1  # 1=Monday, 7=Sunday\n",
    "    start_hour = hourly[\"timestart\"].dt.hour.astype(\"int8\")\n",
    "    end_hour = hourly[\"timeend\"].dt.hour.astype(\"int8\")\n",
    "    \n",
    "    # ---------- Add night time indicator (10pm-5am is hours 22, 23, 0, 1, 2, 3, 4) ----------\n",
    "    is_night = ((start_hour >= 22) | (start_hour <= 4)).astype(\"int8\")\n",
    "    \n",
    "    hourly = hourly.assign(\n",
    "        month=month_num,\n",
    "        day_of_week=day_of_week,\n",
    "        start_hour=start_hour,\n",
    "        end_hour=end_hour,\n",
    "        is_night=is_night\n",
    "    )\n",
    "\n",
    "    # ---------- Add lag features for up to 3 hours ----------\n",
    "    hourly = hourly.sort_values([\"station_id\", \"timestart\"]).reset_index(drop=True)\n",
    "    \n",
    "    # 1-hour lag\n",
    "    hourly[\"last_hour_in\"] = hourly.groupby(\"station_id\")[\"in\"].shift(1)\n",
    "    hourly[\"last_hour_out\"] = hourly.groupby(\"station_id\")[\"out\"].shift(1)\n",
    "    \n",
    "    # 2-hour lag\n",
    "    hourly[\"last_two_hour_in\"] = hourly.groupby(\"station_id\")[\"in\"].shift(2)\n",
    "    hourly[\"last_two_hour_out\"] = hourly.groupby(\"station_id\")[\"out\"].shift(2)\n",
    "    \n",
    "    # 3-hour lag\n",
    "    hourly[\"last_three_hour_in\"] = hourly.groupby(\"station_id\")[\"in\"].shift(3)\n",
    "    hourly[\"last_three_hour_out\"] = hourly.groupby(\"station_id\")[\"out\"].shift(3)\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    hourly[\"last_hour_in\"] = hourly[\"last_hour_in\"].fillna(0).astype(\"int64\")\n",
    "    hourly[\"last_hour_out\"] = hourly[\"last_hour_out\"].fillna(0).astype(\"int64\")\n",
    "    hourly[\"last_two_hour_in\"] = hourly[\"last_two_hour_in\"].fillna(0).astype(\"int64\")\n",
    "    hourly[\"last_two_hour_out\"] = hourly[\"last_two_hour_out\"].fillna(0).astype(\"int64\")\n",
    "    hourly[\"last_three_hour_in\"] = hourly[\"last_three_hour_in\"].fillna(0).astype(\"int64\")\n",
    "    hourly[\"last_three_hour_out\"] = hourly[\"last_three_hour_out\"].fillna(0).astype(\"int64\")\n",
    "\n",
    "    # ---------- Final column order ----------\n",
    "    hourly = (\n",
    "        hourly.loc[:, [\n",
    "            \"timestart\", \"timeend\",\n",
    "            \"station_name\",\n",
    "            \"month\", \"day_of_week\", \"start_hour\", \"end_hour\", \"is_night\",\n",
    "            \"station_id\", \"latitude\", \"longitude\",\n",
    "            \"in\", \"out\",\n",
    "            \"last_hour_in\", \"last_hour_out\",\n",
    "            \"last_two_hour_in\", \"last_two_hour_out\",\n",
    "            \"last_three_hour_in\", \"last_three_hour_out\"\n",
    "        ]]\n",
    "        .sort_values([\"timestart\", \"timeend\", \"station_name\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30450,
     "status": "ok",
     "timestamp": 1761603498657,
     "user": {
      "displayName": "Matthew He Yan",
      "userId": "17059746695771188026"
     },
     "user_tz": 240
    },
    "id": "LfTXvVKrbNtI",
    "outputId": "bb33e200-2ca2-4490-d2c4-f4e235439184"
   },
   "outputs": [],
   "source": [
    "files = [\n",
    "    # \"202301-bluebikes-tripdata\",\n",
    "    # \"202302-bluebikes-tripdata\",\n",
    "    # \"202303-bluebikes-tripdata\",\n",
    "    \"202304-bluebikes-tripdata\",\n",
    "    \"202305-bluebikes-tripdata\",\n",
    "    \"202306-bluebikes-tripdata\",\n",
    "    \"202307-bluebikes-tripdata\",\n",
    "    \"202308-bluebikes-tripdata\",\n",
    "    \"202309-bluebikes-tripdata\",\n",
    "    \"202310-bluebikes-tripdata\",\n",
    "    \"202311-bluebikes-tripdata\",\n",
    "    \"202312-bluebikes-tripdata\",\n",
    "]\n",
    "\n",
    "paths = [input_dir / f\"{file}.csv\" for file in files]\n",
    "\n",
    "# ---------- Process each file separately ----------\n",
    "all_transformed_data = []\n",
    "for file, path in zip(files, paths):\n",
    "    # Load\n",
    "    df_raw = load_raw_data(path)\n",
    "\n",
    "    # Transform\n",
    "    df_transformed = transform_data(df_raw)\n",
    "    \n",
    "    # Filter out rows with more than 50 in any of the traffic columns\n",
    "    df_transformed = df_transformed[\n",
    "        (df_transformed['in'] <= 50) & \n",
    "        (df_transformed['out'] <= 50) & \n",
    "        (df_transformed['last_hour_in'] <= 50) & \n",
    "        (df_transformed['last_hour_out'] <= 50) &\n",
    "        (df_transformed['last_two_hour_in'] <= 50) & \n",
    "        (df_transformed['last_two_hour_out'] <= 50) &\n",
    "        (df_transformed['last_three_hour_in'] <= 50) & \n",
    "        (df_transformed['last_three_hour_out'] <= 50)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Processed {file}: {len(df_transformed)} rows\")\n",
    "    all_transformed_data.append(df_transformed)\n",
    "\n",
    "    # Save one output per input file\n",
    "    out_path = output_dir / f\"{file}_parsed.csv\"\n",
    "    df_transformed.to_csv(out_path)\n",
    "\n",
    "# Concatenate all transformed data\n",
    "df_transformed = pd.concat(all_transformed_data, ignore_index=True)\n",
    "print(f\"\\nTotal combined rows: {len(df_transformed)}\")\n",
    "print(df_transformed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global distribution of in and out counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distribution of 'in' and 'out' values\n",
    "in_distribution = df_transformed['in'].value_counts().sort_index()\n",
    "out_distribution = df_transformed['out'].value_counts().sort_index()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 'in' values\n",
    "ax1.bar(in_distribution.index, in_distribution.values, color='steelblue', edgecolor='black')\n",
    "ax1.set_xlabel('In Value', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Distribution of In Values', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 'out' values\n",
    "ax2.bar(out_distribution.index, out_distribution.values, color='coral', edgecolor='black')\n",
    "ax2.set_xlabel('Out Value', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Distribution of Out Values', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 most used stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total activity (ins + outs) for each station\n",
    "station_activity = df_transformed.groupby('station_name').agg({\n",
    "    'in': 'sum',\n",
    "    'out': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate total activity\n",
    "station_activity['total_activity'] = station_activity['in'] + station_activity['out']\n",
    "\n",
    "# Get top 20 stations by total activity\n",
    "top_20_stations = station_activity.nlargest(20, 'total_activity').reset_index(drop=True)\n",
    "\n",
    "print(f\"Top 20 Most Used Stations:\\n\")\n",
    "print(top_20_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of in and out counts per station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to only include top 20 stations\n",
    "top_20_station_names = top_20_stations['station_name'].tolist()\n",
    "df_top_20 = df_transformed[df_transformed['station_name'].isin(top_20_station_names)].copy()\n",
    "\n",
    "# Calculate average in and out values per hour for each station\n",
    "avg_stats = df_top_20.groupby('station_name').agg({\n",
    "    'in': 'mean',\n",
    "    'out': 'mean'\n",
    "}).reset_index()\n",
    "avg_stats.columns = ['station_name', 'avg_in_per_hour', 'avg_out_per_hour']\n",
    "\n",
    "# Plot distribution of in and out values for each of the top 20 stations\n",
    "for idx, station_name in enumerate(top_20_station_names, 1):\n",
    "    station_data = df_top_20[df_top_20['station_name'] == station_name]\n",
    "    \n",
    "    # Calculate distributions\n",
    "    in_dist = station_data['in'].value_counts().sort_index()\n",
    "    out_dist = station_data['out'].value_counts().sort_index()\n",
    "    \n",
    "    # Get average values for title\n",
    "    avg_in = station_data['in'].mean()\n",
    "    avg_out = station_data['out'].mean()\n",
    "    \n",
    "    # Create figure with 2 subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Plot In distribution\n",
    "    ax1.bar(in_dist.index, in_dist.values, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax1.set_xlabel('In Value', fontsize=11)\n",
    "    ax1.set_ylabel('Frequency', fontsize=11)\n",
    "    ax1.set_title(f'In Distribution - Avg: {avg_in:.2f}', fontsize=12, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot Out distribution\n",
    "    ax2.bar(out_dist.index, out_dist.values, color='coral', edgecolor='black', alpha=0.7)\n",
    "    ax2.set_xlabel('Out Value', fontsize=11)\n",
    "    ax2.set_ylabel('Frequency', fontsize=11)\n",
    "    ax2.set_title(f'Out Distribution - Avg: {avg_out:.2f}', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Overall title\n",
    "    fig.suptitle(f'#{idx}: {station_name}', fontsize=14, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of in and out counts per station (morning and evening rush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from timestart\n",
    "df_top_20['hour'] = df_top_20['timestart'].dt.hour\n",
    "\n",
    "# Define time intervals\n",
    "morning_rush = (df_top_20['hour'] >= 7) & (df_top_20['hour'] < 9)\n",
    "evening_rush = (df_top_20['hour'] >= 17) & (df_top_20['hour'] < 19)\n",
    "\n",
    "# Filter data for each time interval\n",
    "df_morning = df_top_20[morning_rush].copy()\n",
    "df_evening = df_top_20[evening_rush].copy()\n",
    "\n",
    "# Plot in and out distributions for each station during both time periods\n",
    "for idx, station_name in enumerate(top_20_station_names, 1):\n",
    "    # Filter data for this station\n",
    "    morning_data = df_morning[df_morning['station_name'] == station_name]\n",
    "    evening_data = df_evening[df_evening['station_name'] == station_name]\n",
    "    \n",
    "    # Calculate distributions for morning\n",
    "    morning_in_dist = morning_data['in'].value_counts().sort_index()\n",
    "    morning_out_dist = morning_data['out'].value_counts().sort_index()\n",
    "    \n",
    "    # Calculate distributions for evening\n",
    "    evening_in_dist = evening_data['in'].value_counts().sort_index()\n",
    "    evening_out_dist = evening_data['out'].value_counts().sort_index()\n",
    "    \n",
    "    # Get average values for titles\n",
    "    avg_morning_in = morning_data['in'].mean()\n",
    "    avg_morning_out = morning_data['out'].mean()\n",
    "    avg_evening_in = evening_data['in'].mean()\n",
    "    avg_evening_out = evening_data['out'].mean()\n",
    "    \n",
    "    # Create figure with 4 subplots (2x2 grid)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Plot morning rush In (7am-9am)\n",
    "    axes[0, 0].bar(morning_in_dist.index, morning_in_dist.values, \n",
    "                   color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('In Value', fontsize=11)\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[0, 0].set_title(f'Morning In (7am-9am) - Avg: {avg_morning_in:.2f}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot morning rush Out (7am-9am)\n",
    "    axes[0, 1].bar(morning_out_dist.index, morning_out_dist.values, \n",
    "                   color='coral', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Out Value', fontsize=11)\n",
    "    axes[0, 1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[0, 1].set_title(f'Morning Out (7am-9am) - Avg: {avg_morning_out:.2f}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot evening rush In (5pm-7pm)\n",
    "    axes[1, 0].bar(evening_in_dist.index, evening_in_dist.values, \n",
    "                   color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('In Value', fontsize=11)\n",
    "    axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[1, 0].set_title(f'Evening In (5pm-7pm) - Avg: {avg_evening_in:.2f}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot evening rush Out (5pm-7pm)\n",
    "    axes[1, 1].bar(evening_out_dist.index, evening_out_dist.values, \n",
    "                   color='coral', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Out Value', fontsize=11)\n",
    "    axes[1, 1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[1, 1].set_title(f'Evening Out (5pm-7pm) - Avg: {avg_evening_out:.2f}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Overall title\n",
    "    fig.suptitle(f'#{idx}: {station_name} - Rush Hour In/Out Distribution', \n",
    "                 fontsize=14, fontweight='bold', y=1.00)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (fuck it) ZINB Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load feature datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rapid_transit_stations = pd.read_csv(\"./mbta_stations/Rapid_Transit_Stops.csv\")\n",
    "rapid_transit_stations = rapid_transit_stations[[\"stop_name\", \"stop_lat\", \"stop_lon\", \"stop_address\"]]\n",
    "\n",
    "commuter_rail_stations = pd.read_csv(\"./mbta_stations/Commuter_Rail_Stops.csv\")\n",
    "commuter_rail_stations = commuter_rail_stations[[\"stop_name\", \"stop_lat\", \"stop_lon\", \"stop_address\"]]\n",
    "\n",
    "subway_stations = pd.concat([rapid_transit_stations, commuter_rail_stations], ignore_index=True)\n",
    "\n",
    "bus_stations = pd.read_csv(\"./mbta_stations/Bus_Stops.csv\")\n",
    "bus_stations = bus_stations[[\"stop_name\", \"stop_lat\", \"stop_lon\", \"stop_address\"]]\n",
    "\n",
    "colleges = pd.read_csv(\"./mbta_stations/Universities.csv\")\n",
    "colleges = colleges[[\"NAME\", \"LAT\", \"LON\", \"STREET\", \"CITY\", \"STATE\", \"ZIP\", \"NMCNTY\"]]\n",
    "\n",
    "target_counties = [\"Suffolk County\", \"Middlesex County\", \"Essex County\", \"Norfolk County\", \"Plymouth County\"]\n",
    "colleges = colleges[colleges[\"NMCNTY\"].isin(target_counties)]\n",
    "\n",
    "colleges[\"Address\"] = colleges[\"STREET\"] + \", \" + colleges[\"CITY\"]\n",
    "\n",
    "colleges = colleges.rename(columns={\n",
    "    \"NAME\": \"Name\",\n",
    "    \"LAT\": \"Latitude\", \n",
    "    \"LON\": \"Longitude\"\n",
    "})\n",
    "\n",
    "colleges = colleges[[\"Name\", \"Latitude\", \"Longitude\", \"Address\"]]\n",
    "\n",
    "weather_files = ['202304-weather.csv', '202305-weather.csv', '202306-weather.csv', '202307-weather.csv', '202308-weather.csv', '202309-weather.csv', '202310-weather.csv', '202311-weather.csv', '202312-weather.csv']\n",
    "weather_data = pd.DataFrame()\n",
    "for weather_file in weather_files:\n",
    "    month_data = pd.read_csv(weather_dir / weather_file, skipinitialspace=True)\n",
    "    weather_data = pd.concat([weather_data, month_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_20_busiest_stations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    station_activity = df.groupby('station_id').agg({\n",
    "        'in': 'sum',\n",
    "        'out': 'sum',\n",
    "        'station_name': 'first',\n",
    "        'latitude': 'first',\n",
    "        'longitude': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    station_activity['total_activity'] = station_activity['in'] + station_activity['out']\n",
    "\n",
    "    top_20_stations = station_activity.nlargest(20, 'total_activity').reset_index(drop=True)\n",
    "    \n",
    "    return top_20_stations\n",
    "\n",
    "def extract_bluebike_station_lat_long(df: pd.DataFrame ) -> list:\n",
    "    stations = df[[\"station_id\", \"station_name\", \"latitude\", \"longitude\"]].drop_duplicates(\"station_id\")\n",
    "    stations = stations.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "    \n",
    "    bluebike_station_lat_long = []\n",
    "    for _, row in stations.iterrows():\n",
    "        station_info = {\n",
    "            \"station_id\": row[\"station_id\"],\n",
    "            \"name\": row[\"station_name\"],\n",
    "            \"lat\": row[\"latitude\"],\n",
    "            \"lon\": row[\"longitude\"]\n",
    "        }\n",
    "        bluebike_station_lat_long.append(station_info)\n",
    "    \n",
    "    return bluebike_station_lat_long\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees) using Haversine formula\n",
    "    Returns distance in meters\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    \n",
    "    # Radius of earth in kilometers\n",
    "    r = 6371\n",
    "    \n",
    "    # Return distance in meters\n",
    "    return c * r * 1000\n",
    "\n",
    "\n",
    "def find_closest_subway(bluebike_station_lat_long: list) -> pd.DataFrame:\n",
    "    closest_mbta_results = []\n",
    "\n",
    "    for bluebike_station in bluebike_station_lat_long:\n",
    "        bluebike_lat = bluebike_station['lat']\n",
    "        bluebike_lon = bluebike_station['lon']\n",
    "        bluebike_name = bluebike_station['name']\n",
    "        bluebike_id = bluebike_station['station_id']\n",
    "        \n",
    "        min_distance = float('inf')\n",
    "        closest_mbta = None\n",
    "        \n",
    "        # Calculate distance to each MBTA station\n",
    "        for _, mbta_row in subway_stations.iterrows():\n",
    "            mbta_lat = mbta_row['stop_lat']\n",
    "            mbta_lon = mbta_row['stop_lon']\n",
    "            \n",
    "            # Skip if coordinates are missing\n",
    "            if pd.isna(mbta_lat) or pd.isna(mbta_lon):\n",
    "                continue\n",
    "                \n",
    "            distance = calculate_distance(bluebike_lat, bluebike_lon, mbta_lat, mbta_lon)\n",
    "            \n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_mbta = {\n",
    "                    'name': mbta_row['stop_name'],\n",
    "                    'address': mbta_row['stop_address'],\n",
    "                    'lat': mbta_lat,\n",
    "                    'lon': mbta_lon,\n",
    "                    'distance_m': distance\n",
    "                }\n",
    "        \n",
    "        # Add result to list\n",
    "        if closest_mbta:\n",
    "            result = {\n",
    "                'station_name': bluebike_name,\n",
    "                'station_id': bluebike_id,\n",
    "                'station_latitude': bluebike_lat,\n",
    "                'station_longitude': bluebike_lon,\n",
    "                'closest_subway_name': closest_mbta['name'],\n",
    "                'closest_subway_address': closest_mbta['address'],\n",
    "                'closest_subway_latitude': closest_mbta['lat'],\n",
    "                'closest_subway_longitude': closest_mbta['lon'],\n",
    "                'subway_distance_m': round(closest_mbta['distance_m'], 3)\n",
    "            }\n",
    "            closest_mbta_results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(closest_mbta_results)\n",
    "            \n",
    "            \n",
    "def find_closet_bus_station(bluebike_station_lat_long: list) -> pd.DataFrame:\n",
    "    closest_bus_results = []\n",
    "\n",
    "    for bluebike_station in bluebike_station_lat_long:\n",
    "        bluebike_lat = bluebike_station['lat']\n",
    "        bluebike_lon = bluebike_station['lon']\n",
    "        bluebike_name = bluebike_station['name']\n",
    "        bluebike_id = bluebike_station['station_id']\n",
    "        \n",
    "        min_distance = float('inf')\n",
    "        closest_bus = None\n",
    "        \n",
    "        # Calculate distance to each bus station\n",
    "        for _, bus_row in bus_stations.iterrows():\n",
    "            bus_lat = bus_row['stop_lat']\n",
    "            bus_lon = bus_row['stop_lon']\n",
    "            \n",
    "            # Skip if coordinates are missing\n",
    "            if pd.isna(bus_lat) or pd.isna(bus_lon):\n",
    "                continue\n",
    "                \n",
    "            distance = calculate_distance(bluebike_lat, bluebike_lon, bus_lat, bus_lon)\n",
    "            \n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_bus = {\n",
    "                    'name': bus_row['stop_name'],\n",
    "                    'address': bus_row['stop_address'],\n",
    "                    'lat': bus_lat,\n",
    "                    'lon': bus_lon,\n",
    "                    'distance_m': distance\n",
    "                }\n",
    "        \n",
    "        # Add result to list\n",
    "        if closest_bus:\n",
    "            result = {\n",
    "                'station_name': bluebike_name,\n",
    "                'station_id': bluebike_id,\n",
    "                'station_latitude': bluebike_lat,\n",
    "                'station_longitude': bluebike_lon,\n",
    "                'closest_bus_name': closest_bus['name'],\n",
    "                'closest_bus_address': closest_bus['address'],\n",
    "                'closest_bus_latitude': closest_bus['lat'],\n",
    "                'closest_bus_longitude': closest_bus['lon'],\n",
    "                'bus_distance_m': round(closest_bus['distance_m'], 3)\n",
    "            }\n",
    "            closest_bus_results.append(result)\n",
    "            \n",
    "    return pd.DataFrame(closest_bus_results)\n",
    "\n",
    "def find_nearest_university(bluebike_station_lat_long: list) -> pd.DataFrame:\n",
    "    closest_university_results = []\n",
    "\n",
    "    for bluebike_station in bluebike_station_lat_long:\n",
    "        bluebike_lat = bluebike_station['lat']\n",
    "        bluebike_lon = bluebike_station['lon']\n",
    "        bluebike_name = bluebike_station['name']\n",
    "        bluebike_id = bluebike_station['station_id']\n",
    "        \n",
    "        min_distance = float('inf')\n",
    "        closest_university = None\n",
    "        \n",
    "        # Calculate distance to each university\n",
    "        for _, uni_row in colleges.iterrows():\n",
    "            uni_lat = uni_row['Latitude']\n",
    "            uni_lon = uni_row['Longitude']\n",
    "            \n",
    "            # Skip if coordinates are missing\n",
    "            if pd.isna(uni_lat) or pd.isna(uni_lon):\n",
    "                continue\n",
    "                \n",
    "            distance = calculate_distance(bluebike_lat, bluebike_lon, uni_lat, uni_lon)\n",
    "            \n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_university = {\n",
    "                    'name': uni_row['Name'],\n",
    "                    'address': uni_row['Address'],\n",
    "                    'lat': uni_lat,\n",
    "                    'lon': uni_lon,\n",
    "                    'distance_m': distance\n",
    "                }\n",
    "        \n",
    "        # Add result to list\n",
    "        if closest_university:\n",
    "            result = {\n",
    "                'station_name': bluebike_name,\n",
    "                'station_id': bluebike_id,\n",
    "                'station_latitude': bluebike_lat,\n",
    "                'station_longitude': bluebike_lon,\n",
    "                'closest_university_name': closest_university['name'],\n",
    "                'closest_university_address': closest_university['address'],\n",
    "                'closest_university_latitude': closest_university['lat'],\n",
    "                'closest_university_longitude': closest_university['lon'],\n",
    "                'university_distance_m': round(closest_university['distance_m'], 3)\n",
    "            }\n",
    "            closest_university_results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(closest_university_results)\n",
    "\n",
    "def count_nearby_subway_stations(bluebike_station_lat_long: list, radius_m: float = 250) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Count the number of MBTA subway/commuter rail stations within a specified radius\n",
    "    of each BlueBike station.\n",
    "    \"\"\"\n",
    "    count_results = []\n",
    "    \n",
    "    for bluebike_station in bluebike_station_lat_long:\n",
    "        bluebike_lat = bluebike_station['lat']\n",
    "        bluebike_lon = bluebike_station['lon']\n",
    "        bluebike_id = bluebike_station['station_id']\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        # Count MBTA stations within radius\n",
    "        for _, mbta_row in subway_stations.iterrows():\n",
    "            mbta_lat = mbta_row['stop_lat']\n",
    "            mbta_lon = mbta_row['stop_lon']\n",
    "            \n",
    "            # Skip if coordinates are missing\n",
    "            if pd.isna(mbta_lat) or pd.isna(mbta_lon):\n",
    "                continue\n",
    "                \n",
    "            distance = calculate_distance(bluebike_lat, bluebike_lon, mbta_lat, mbta_lon)\n",
    "            \n",
    "            if distance <= radius_m:\n",
    "                count += 1\n",
    "        \n",
    "        result = {\n",
    "            'station_id': bluebike_id,\n",
    "            'mbta_stops_250m': count\n",
    "        }\n",
    "        count_results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(count_results)\n",
    "\n",
    "def count_nearby_bus_stops(bluebike_station_lat_long: list, radius_m: float = 250) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Count the number of bus stops within a specified radius of each BlueBike station.\n",
    "    \"\"\"\n",
    "    count_results = []\n",
    "    \n",
    "    for bluebike_station in bluebike_station_lat_long:\n",
    "        bluebike_lat = bluebike_station['lat']\n",
    "        bluebike_lon = bluebike_station['lon']\n",
    "        bluebike_id = bluebike_station['station_id']\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        # Count bus stops within radius\n",
    "        for _, bus_row in bus_stations.iterrows():\n",
    "            bus_lat = bus_row['stop_lat']\n",
    "            bus_lon = bus_row['stop_lon']\n",
    "            \n",
    "            # Skip if coordinates are missing\n",
    "            if pd.isna(bus_lat) or pd.isna(bus_lon):\n",
    "                continue\n",
    "                \n",
    "            distance = calculate_distance(bluebike_lat, bluebike_lon, bus_lat, bus_lon)\n",
    "            \n",
    "            if distance <= radius_m:\n",
    "                count += 1\n",
    "        \n",
    "        result = {\n",
    "            'station_id': bluebike_id,\n",
    "            'bus_stops_250m': count\n",
    "        }\n",
    "        count_results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(count_results)\n",
    "\n",
    "def transform_weather_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df['precipitation'] = df['precipitation'].replace('T', '0.0')\n",
    "    df['precipitation'] = pd.to_numeric(df['precipitation'], errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract top 20 busiest stations\n",
    "top_20_stations = extract_top_20_busiest_stations(df_transformed)\n",
    "print(f\"Extracted {len(top_20_stations)} busiest stations\")\n",
    "print()\n",
    "\n",
    "# Step 2: Get BlueBike station coordinates for top 20\n",
    "top_20_station_ids = top_20_stations['station_id'].tolist()\n",
    "top_20_coords = extract_bluebike_station_lat_long(df_transformed)\n",
    "top_20_coords = [s for s in top_20_coords if s['station_id'] in top_20_station_ids]\n",
    "print(f\"Got coordinates for {len(top_20_coords)} stations\")\n",
    "print()\n",
    "\n",
    "# Step 3: Find closest features using helper functions\n",
    "print(\"Finding closest universities...\")\n",
    "nearest_universities = find_nearest_university(top_20_coords)\n",
    "print(f\"Found {len(nearest_universities)} university matches\")\n",
    "print()\n",
    "\n",
    "print(\"Finding closest subway/commuter rail stations...\")\n",
    "nearest_subway = find_closest_subway(top_20_coords)\n",
    "print(f\"Found {len(nearest_subway)} subway matches\")\n",
    "print()\n",
    "\n",
    "print(\"Finding closest bus stations...\")\n",
    "nearest_bus = find_closet_bus_station(top_20_coords)\n",
    "print(f\"Found {len(nearest_bus)} bus matches\")\n",
    "print()\n",
    "\n",
    "print(\"Counting nearby MBTA stops (within 250m)...\")\n",
    "mbta_counts = count_nearby_subway_stations(top_20_coords, radius_m=250)\n",
    "print(f\"Computed counts for {len(mbta_counts)} stations\")\n",
    "print()\n",
    "\n",
    "print(\"Counting nearby bus stops (within 250m)...\")\n",
    "bus_counts = count_nearby_bus_stops(top_20_coords, radius_m=250)\n",
    "print(f\"Computed counts for {len(bus_counts)} stations\")\n",
    "print()\n",
    "\n",
    "print(\"Transforming weather data...\")\n",
    "weather_features = transform_weather_data(weather_data)\n",
    "print(f\"Transformed {len(weather_features)} lines\")\n",
    "print()\n",
    "\n",
    "# Step 4: Merge the feature DataFrames\n",
    "# Start with universities\n",
    "features_df = nearest_universities[['station_id', 'closest_university_name', \n",
    "                                     'closest_university_address', 'closest_university_latitude',\n",
    "                                     'closest_university_longitude', 'university_distance_m']]\n",
    "\n",
    "# Merge subway data\n",
    "subway_features = nearest_subway[['station_id', 'closest_subway_name',\n",
    "                                   'closest_subway_address', 'closest_subway_latitude',\n",
    "                                   'closest_subway_longitude', 'subway_distance_m']]\n",
    "features_df = features_df.merge(subway_features, on='station_id', how='outer')\n",
    "\n",
    "# Merge bus data\n",
    "bus_features = nearest_bus[['station_id', 'closest_bus_name',\n",
    "                             'closest_bus_address', 'closest_bus_latitude',\n",
    "                             'closest_bus_longitude', 'bus_distance_m']]\n",
    "features_df = features_df.merge(bus_features, on='station_id', how='outer')\n",
    "\n",
    "# Merge MBTA and bus counts\n",
    "features_df = features_df.merge(mbta_counts, on='station_id', how='outer')\n",
    "features_df = features_df.merge(bus_counts, on='station_id', how='outer')\n",
    "\n",
    "# Step 5: Filter df_transformed to top 20 stations and join features\n",
    "df_top_20_enriched = df_transformed[df_transformed['station_id'].isin(top_20_station_ids)].copy()\n",
    "df_top_20_enriched = df_top_20_enriched.merge(features_df, on='station_id', how='left')\n",
    "\n",
    "# Step 6: Extract date from timestart and merge weather data\n",
    "df_top_20_enriched['date'] = df_top_20_enriched['timestart'].dt.date\n",
    "df_top_20_enriched['date'] = pd.to_datetime(df_top_20_enriched['date'])\n",
    "df_top_20_enriched = df_top_20_enriched.merge(weather_features, on='date', how='left')\n",
    "\n",
    "\n",
    "print(f\"Final enriched dataframe shape: {df_top_20_enriched.shape}\")\n",
    "print(f\"\\nColumns in enriched dataframe:\")\n",
    "print(df_top_20_enriched.columns.tolist())\n",
    "print()\n",
    "\n",
    "print(\"Sample of enriched data:\")\n",
    "print(df_top_20_enriched.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZINB Regression Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.count_model import ZeroInflatedNegativeBinomialP\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for ZINB regression\n",
    "# Select numerical features from df_top_20_enriched\n",
    "feature_cols = [\n",
    "    'month', 'day_of_week', \n",
    "    'start_hour', 'end_hour',\n",
    "    'university_distance_m',\n",
    "    'subway_distance_m',\n",
    "    'bus_distance_m',\n",
    "    'mbta_stops_250m',\n",
    "    'bus_stops_250m',\n",
    "    'last_hour_in',\n",
    "    'last_hour_out',\n",
    "    'last_two_hour_in',\n",
    "    'last_two_hour_out',\n",
    "    'last_three_hour_in',\n",
    "    'last_three_hour_out',\n",
    "    'is_night',\n",
    "    'avg_temp',\n",
    "    'precipitation'\n",
    "]\n",
    "\n",
    "# Define negative binomial features\n",
    "nb_features = [\n",
    "    'month',\n",
    "    'start_hour',\n",
    "    'end_hour',\n",
    "    'bus_distance_m',\n",
    "    'last_three_hour_in',\n",
    "    'last_three_hour_out',\n",
    "]\n",
    "\n",
    "# Define inflation features\n",
    "infl_features = ['is_night', 'precipitation', 'avg_temp']\n",
    "\n",
    "# Convert all columns to numeric, coercing errors to NaN\n",
    "for col in feature_cols:\n",
    "    df_top_20_enriched[col] = pd.to_numeric(df_top_20_enriched[col], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values only in feature columns\n",
    "df_top_20_enriched = df_top_20_enriched.dropna(subset=feature_cols).reset_index(drop=True)\n",
    "\n",
    "# Prepare data - ensure all numeric types and handle missing values\n",
    "X = df_top_20_enriched[feature_cols].copy().values\n",
    "y_out = df_top_20_enriched['out'].values\n",
    "y_in = df_top_20_enriched['in'].values\n",
    "\n",
    "# Split into train and test sets - use the SAME indices for both targets\n",
    "indices = np.arange(len(X))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X[train_idx])\n",
    "X_test = scaler.transform(X[test_idx])\n",
    "\n",
    "y_out_train = y_out[train_idx]\n",
    "y_out_test = y_out[test_idx]\n",
    "y_in_train = y_in[train_idx]\n",
    "y_in_test = y_in[test_idx]\n",
    "\n",
    "# Add constant term for intercept\n",
    "X_train_const = sm.add_constant(X_train[:, [feature_cols.index(f) for f in nb_features]])\n",
    "X_train_infl = sm.add_constant(X_train[:, [feature_cols.index(f) for f in infl_features]])\n",
    "X_test_const = sm.add_constant(X_test[:, [feature_cols.index(f) for f in nb_features]])\n",
    "X_test_infl = sm.add_constant(X_test[:, [feature_cols.index(f) for f in infl_features]])\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "print(f\"Negative binomial features: {nb_features}\")\n",
    "print(f\"Inflation features: {infl_features}\")\n",
    "print(f\"Features shape (with constant): {X_train_const.shape}\")\n",
    "print(f\"X_train_const dtype: {X_train_const.dtype}\")\n",
    "print(f\"y_out_train dtype: {y_out_train.dtype}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 1: ZINB for OUT counts\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL 1: Fitting ZINB for OUT Counts\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Fit the model - both count and inflation models use the same features\n",
    "zinb_out_model = ZeroInflatedNegativeBinomialP(\n",
    "    endog=y_out_train,\n",
    "    exog=X_train_const,          # Features for count model (predicts μ)\n",
    "    exog_infl=X_train_infl,     # Features for inflation model (predicts π)\n",
    "    p=2                           # NB-P parameterization (p=2 is standard NB2)\n",
    ")\n",
    "\n",
    "# Fit with maximum likelihood\n",
    "zinb_out_results = zinb_out_model.fit(method='bfgs', maxiter=1000, disp=True)\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"OUT Model Summary\")\n",
    "print(\"=\"*70)\n",
    "print(zinb_out_results.summary())\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: ZINB for IN counts\n",
    "# ============================================================================\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL 2: Fitting ZINB for IN Counts\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Fit the model - both count and inflation models use the same features\n",
    "zinb_in_model = ZeroInflatedNegativeBinomialP(\n",
    "    endog=y_in_train,\n",
    "    exog=X_train_const,          # Features for count model (predicts μ)\n",
    "    exog_infl=X_train_infl,     # Features for inflation model (predicts π)\n",
    "    p=2                           # NB-P parameterization (p=2 is standard NB2)\n",
    ")\n",
    "\n",
    "# Fit with maximum likelihood\n",
    "zinb_in_results = zinb_in_model.fit(method='bfgs', maxiter=1000, disp=True)\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"IN Model Summary\")\n",
    "print(\"=\"*70)\n",
    "print(zinb_in_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREDICTIONS AND EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"Test Set Predictions - OUT Model\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# OUT predictions\n",
    "# y_out_pred = zinb_out_results.predict(exog=X_test_const, exog_infl=X_test_infl, which='mean')\n",
    "# pi_out_pred = zinb_out_results.predict(exog=X_test_const, exog_infl=X_test_infl, which='prob-zero')\n",
    "# mu_out_pred = zinb_out_results.predict(exog=X_test_const, exog_infl=X_test_infl, which='mean-main')\n",
    "\n",
    "# Modified OUT predictions\n",
    "y_out_pred_original = zinb_out_results.predict(exog=X_test_const, exog_infl=X_test_infl, which='mean')\n",
    "pi_out_pred = zinb_out_results.predict(exog=X_test_const, exog_infl=X_test_infl, which='prob-zero')\n",
    "mu_out_pred = zinb_out_results.predict(exog=X_test_const, exog_infl=X_test_infl, which='mean-main')\n",
    "\n",
    "# Apply rule: if pi > 0.5, set prediction to 0\n",
    "y_out_pred = np.round(np.where(pi_out_pred > 0.5, 0, y_out_pred_original))\n",
    "\n",
    "# Evaluate OUT predictions\n",
    "mse_out = mean_squared_error(y_out_test, y_out_pred)\n",
    "mae_out = mean_absolute_error(y_out_test, y_out_pred)\n",
    "rmse_out = np.sqrt(mse_out)\n",
    "r2_out = r2_score(y_out_test, y_out_pred)\n",
    "\n",
    "alpha_idx = len(zinb_out_results.params) - 1\n",
    "alpha = zinb_out_results.params[alpha_idx]\n",
    "\n",
    "print(f\"OUT Prediction Metrics:\")\n",
    "print(f\"  RMSE: {rmse_out:.4f}\")\n",
    "print(f\"  MAE: {mae_out:.4f}\")\n",
    "print(f\"  MSE: {mse_out:.4f}\")\n",
    "print(f\"  R²: {r2_out:.4f}\")\n",
    "print()\n",
    "\n",
    "print(f\"OUT Distribution Parameter Statistics:\")\n",
    "print(f\"  Zero-inflation probability π:\")\n",
    "print(f\"    Mean: {pi_out_pred.mean():.4f}\")\n",
    "print(f\"    Std:  {pi_out_pred.std():.4f}\")\n",
    "print(f\"  NB mean μ (from count model):\")\n",
    "print(f\"    Mean: {mu_out_pred.mean():.4f}\")\n",
    "print(f\"    Std:  {mu_out_pred.std():.4f}\")\n",
    "print(f\"  Dispersion parameter α: {alpha:.4f}\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"Test Set Predictions - IN Model\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# IN predictions\n",
    "# y_in_pred = zinb_in_results.predict(exog=X_test_const, exog_infl=X_test_infl, which='mean')\n",
    "# pi_in_pred = zinb_in_results.predict(exog=X_test_const, exog_infl=X_test_infl, which='prob-zero')\n",
    "# mu_in_pred = zinb_in_results.predict(exog=X_test_const, exog_infl=X_test_infl, which='mean-main')\n",
    "\n",
    "# Modified IN predictions\n",
    "y_in_pred_original = zinb_in_results.predict(exog=X_test_const, exog_infl=X_test_infl, which='mean')\n",
    "pi_in_pred = zinb_in_results.predict(exog=X_test_const, exog_infl=X_test_infl, which='prob-zero')\n",
    "mu_in_pred = zinb_in_results.predict(exog=X_test_const, exog_infl=X_test_infl, which='mean-main')\n",
    "\n",
    "# Apply rule: if pi > 0.5, set prediction to 0\n",
    "y_in_pred = np.round(np.where(pi_in_pred > 0.5, 0, y_in_pred_original))\n",
    "\n",
    "# Evaluate IN predictions\n",
    "mse_in = mean_squared_error(y_in_test, y_in_pred)\n",
    "mae_in = mean_absolute_error(y_in_test, y_in_pred)\n",
    "rmse_in = np.sqrt(mse_in)\n",
    "r2_in = r2_score(y_in_test, y_in_pred)\n",
    "\n",
    "alpha_idx = len(zinb_out_results.params) - 1\n",
    "alpha = zinb_out_results.params[alpha_idx]\n",
    "\n",
    "print(f\"IN Prediction Metrics:\")\n",
    "print(f\"  RMSE: {rmse_in:.4f}\")\n",
    "print(f\"  MAE: {mae_in:.4f}\")\n",
    "print(f\"  MSE: {mse_in:.4f}\")\n",
    "print(f\"  R²: {r2_in:.4f}\")\n",
    "print()\n",
    "\n",
    "print(f\"IN Distribution Parameter Statistics:\")\n",
    "print(f\"  Zero-inflation probability π:\")\n",
    "print(f\"    Mean: {pi_in_pred.mean():.4f}\")\n",
    "print(f\"    Std:  {pi_in_pred.std():.4f}\")\n",
    "print(f\"  NB mean μ (from count model):\")\n",
    "print(f\"    Mean: {mu_in_pred.mean():.4f}\")\n",
    "print(f\"    Std:  {mu_in_pred.std():.4f}\")\n",
    "print(f\"  Dispersion parameter α: {alpha:.4f}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"Feature Importance Comparison\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Recreate feature names (with const)\n",
    "feature_names_with_const = ['const'] + nb_features\n",
    "\n",
    "# OUT Count model coefficients\n",
    "print(\"OUT Count Model Coefficients (predicting μ_out):\")\n",
    "count_params_out = zinb_out_results.params[:len(feature_names_with_const)]\n",
    "count_pvalues_out = zinb_out_results.pvalues[:len(feature_names_with_const)]\n",
    "for i, col in enumerate(feature_names_with_const):\n",
    "    sig = \"***\" if count_pvalues_out[i] < 0.001 else \"**\" if count_pvalues_out[i] < 0.01 else \"*\" if count_pvalues_out[i] < 0.05 else \"\"\n",
    "    print(f\"  {col:30s}: {count_params_out[i]:10.4f}  (p={count_pvalues_out[i]:.4f}) {sig}\")\n",
    "print()\n",
    "\n",
    "# IN Count model coefficients\n",
    "print(\"IN Count Model Coefficients (predicting μ_in):\")\n",
    "count_params_in = zinb_in_results.params[:len(feature_names_with_const)]\n",
    "count_pvalues_in = zinb_in_results.pvalues[:len(feature_names_with_const)]\n",
    "for i, col in enumerate(feature_names_with_const):\n",
    "    sig = \"***\" if count_pvalues_in[i] < 0.001 else \"**\" if count_pvalues_in[i] < 0.01 else \"*\" if count_pvalues_in[i] < 0.05 else \"\"\n",
    "    print(f\"  {col:30s}: {count_params_in[i]:10.4f}  (p={count_pvalues_in[i]:.4f}) {sig}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Row 1: OUT Model\n",
    "# 1. OUT: Actual vs Predicted\n",
    "ax1 = plt.subplot(3, 4, 1)\n",
    "plt.scatter(y_out_test, y_out_pred, alpha=0.3, s=10, edgecolors='none', color='coral')\n",
    "plt.plot([y_out_test.min(), y_out_test.max()], [y_out_test.min(), y_out_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual OUT Counts', fontsize=10)\n",
    "plt.ylabel('Predicted OUT Counts', fontsize=10)\n",
    "plt.title(f'OUT: Actual vs Predicted\\nR² = {r2_out:.4f}', fontsize=11, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. OUT: Residuals\n",
    "ax2 = plt.subplot(3, 4, 2)\n",
    "residuals_out = y_out_test - y_out_pred\n",
    "plt.scatter(y_out_pred, residuals_out, alpha=0.3, s=10, edgecolors='none', color='coral')\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted OUT Counts', fontsize=10)\n",
    "plt.ylabel('Residuals', fontsize=10)\n",
    "plt.title('OUT: Residual Plot', fontsize=11, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. OUT: Distribution of π\n",
    "ax3 = plt.subplot(3, 4, 3)\n",
    "plt.hist(pi_out_pred, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "plt.xlabel('Zero-Inflation Prob (π)', fontsize=10)\n",
    "plt.ylabel('Frequency', fontsize=10)\n",
    "plt.title(f'OUT: Distribution of π\\nMean={pi_out_pred.mean():.3f}', fontsize=11, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. OUT: Distribution of μ\n",
    "ax4 = plt.subplot(3, 4, 4)\n",
    "plt.hist(mu_out_pred, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "plt.xlabel('NB Mean (μ)', fontsize=10)\n",
    "plt.ylabel('Frequency', fontsize=10)\n",
    "plt.title(f'OUT: Distribution of μ\\nMean={mu_out_pred.mean():.3f}', fontsize=11, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 2: IN Model\n",
    "# 5. IN: Actual vs Predicted\n",
    "ax5 = plt.subplot(3, 4, 5)\n",
    "plt.scatter(y_in_test, y_in_pred, alpha=0.3, s=10, edgecolors='none', color='steelblue')\n",
    "plt.plot([y_in_test.min(), y_in_test.max()], [y_in_test.min(), y_in_test.max()], 'b--', lw=2)\n",
    "plt.xlabel('Actual IN Counts', fontsize=10)\n",
    "plt.ylabel('Predicted IN Counts', fontsize=10)\n",
    "plt.title(f'IN: Actual vs Predicted\\nR² = {r2_in:.4f}', fontsize=11, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. IN: Residuals\n",
    "ax6 = plt.subplot(3, 4, 6)\n",
    "residuals_in = y_in_test - y_in_pred\n",
    "plt.scatter(y_in_pred, residuals_in, alpha=0.3, s=10, edgecolors='none', color='steelblue')\n",
    "plt.axhline(y=0, color='b', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted IN Counts', fontsize=10)\n",
    "plt.ylabel('Residuals', fontsize=10)\n",
    "plt.title('IN: Residual Plot', fontsize=11, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. IN: Distribution of π\n",
    "ax7 = plt.subplot(3, 4, 7)\n",
    "plt.hist(pi_in_pred, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "plt.xlabel('Zero-Inflation Prob (π)', fontsize=10)\n",
    "plt.ylabel('Frequency', fontsize=10)\n",
    "plt.title(f'IN: Distribution of π\\nMean={pi_in_pred.mean():.3f}', fontsize=11, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 8. IN: Distribution of μ\n",
    "ax8 = plt.subplot(3, 4, 8)\n",
    "plt.hist(mu_in_pred, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "plt.xlabel('NB Mean (μ)', fontsize=10)\n",
    "plt.ylabel('Frequency', fontsize=10)\n",
    "plt.title(f'IN: Distribution of μ\\nMean={mu_in_pred.mean():.3f}', fontsize=11, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 3: Comparisons\n",
    "# 9. OUT vs IN Predicted Counts\n",
    "ax9 = plt.subplot(3, 4, 9)\n",
    "plt.scatter(y_out_pred, y_in_pred, alpha=0.3, s=10, edgecolors='none', color='purple')\n",
    "min_val = min(y_out_pred.min(), y_in_pred.min())\n",
    "max_val = max(y_out_pred.max(), y_in_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='y=x')\n",
    "plt.xlabel('Predicted OUT Counts', fontsize=10)\n",
    "plt.ylabel('Predicted IN Counts', fontsize=10)\n",
    "plt.title('Predicted OUT vs IN', fontsize=11, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 10. Coefficient comparison for count models\n",
    "ax10 = plt.subplot(3, 4, 10)\n",
    "feature_names_no_const = [col for col in feature_names_with_const if col != 'const']\n",
    "out_coefs = [count_params_out[i] for i, col in enumerate(feature_names_with_const) if col != 'const']\n",
    "in_coefs = [count_params_in[i] for i, col in enumerate(feature_names_with_const) if col != 'const']\n",
    "x_pos = np.arange(len(feature_names_no_const))\n",
    "width = 0.35\n",
    "plt.bar(x_pos - width/2, out_coefs, width, label='OUT', color='coral', alpha=0.7)\n",
    "plt.bar(x_pos + width/2, in_coefs, width, label='IN', color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Features', fontsize=10)\n",
    "plt.ylabel('Coefficient Value', fontsize=10)\n",
    "plt.title('Count Model Coefficients', fontsize=11, fontweight='bold')\n",
    "plt.xticks(x_pos, feature_names_no_const, rotation=45, ha='right', fontsize=8)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# 11. Model Performance Comparison\n",
    "ax11 = plt.subplot(3, 4, 11)\n",
    "metrics = ['RMSE', 'MAE', 'R²']\n",
    "out_metrics = [rmse_out, mae_out, r2_out]\n",
    "in_metrics = [rmse_in, mae_in, r2_in]\n",
    "x_pos = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "plt.bar(x_pos - width/2, out_metrics, width, label='OUT', color='coral', alpha=0.7)\n",
    "plt.bar(x_pos + width/2, in_metrics, width, label='IN', color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Metric', fontsize=10)\n",
    "plt.ylabel('Value', fontsize=10)\n",
    "plt.title('Model Performance Comparison', fontsize=11, fontweight='bold')\n",
    "plt.xticks(x_pos, metrics)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 12. Zero proportions comparison\n",
    "ax12 = plt.subplot(3, 4, 12)\n",
    "zero_prop_out_actual = (y_out_test == 0).mean()\n",
    "zero_prop_out_pred = (y_out_pred < 0.5).mean()\n",
    "zero_prop_in_actual = (y_in_test == 0).mean()\n",
    "zero_prop_in_pred = (y_in_pred < 0.5).mean()\n",
    "categories = ['OUT\\nActual', 'OUT\\nPredicted', 'IN\\nActual', 'IN\\nPredicted']\n",
    "values = [zero_prop_out_actual, zero_prop_out_pred, zero_prop_in_actual, zero_prop_in_pred]\n",
    "colors = ['coral', 'lightsalmon', 'steelblue', 'lightblue']\n",
    "plt.bar(categories, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('Proportion of Zeros', fontsize=10)\n",
    "plt.title('Zero Proportions', fontsize=11, fontweight='bold')\n",
    "plt.ylim([0, max(values) * 1.2])\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', fontsize=9)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "alpha_out_idx = len(zinb_out_results.params) - 1\n",
    "alpha_out = zinb_out_results.params[alpha_out_idx]\n",
    "\n",
    "# Get dispersion parameter for IN model\n",
    "alpha_in_idx = len(zinb_in_results.params) - 1\n",
    "alpha_in = zinb_in_results.params[alpha_in_idx]\n",
    "\n",
    "# Summary comparison\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"Model Comparison Summary\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Calculate prediction accuracy (exact matches after rounding)\n",
    "accuracy_out = (y_out_pred == y_out_test).mean()\n",
    "accuracy_in = (y_in_pred == y_in_test).mean()\n",
    "\n",
    "print(f\"{'Metric':<25} {'OUT Model':<20} {'IN Model':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Overall Accuracy (%)':<25} {accuracy_out*100:<20.2f} {accuracy_in*100:<20.2f}\")\n",
    "print(f\"{'RMSE':<25} {rmse_out:<20.4f} {rmse_in:<20.4f}\")\n",
    "print(f\"{'MAE':<25} {mae_out:<20.4f} {mae_in:<20.4f}\")\n",
    "print(f\"{'R²':<25} {r2_out:<20.4f} {r2_in:<20.4f}\")\n",
    "print(f\"{'Mean π':<25} {pi_out_pred.mean():<20.4f} {pi_in_pred.mean():<20.4f}\")\n",
    "print(f\"{'Mean μ':<25} {mu_out_pred.mean():<20.4f} {mu_in_pred.mean():<20.4f}\")\n",
    "print(f\"{'Dispersion α':<25} {alpha_out:<20.4f} {alpha_in:<20.4f}\")\n",
    "print(f\"{'Actual Zero Prop':<25} {zero_prop_out_actual:<20.4f} {zero_prop_in_actual:<20.4f}\")\n",
    "print(f\"{'Predicted Zero Prop':<25} {zero_prop_out_pred:<20.4f} {zero_prop_in_pred:<20.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOUxGbpJdcqaDvrptPuLaTp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
